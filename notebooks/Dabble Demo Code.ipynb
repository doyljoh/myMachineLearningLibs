{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import library\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ScoringObject(object):\n",
    "    \"\"\"\n",
    "        A functional object to be used to score dabble entries \n",
    "         \n",
    "        Attributes of Class:\n",
    "        configData: Dictionary of configuration data \n",
    "        df_SubData: List of team submissions\n",
    "        df_CompTrueRes: The true competions results\n",
    "        scores: A list of scores that compare submitted results to the true competion results \n",
    "        \n",
    "        Attributes of Class Instance:       \n",
    "            TeamID: pointer to Team ID, use to score individual team submission\n",
    "            Scores: dict of submission score\n",
    "    \"\"\"\n",
    "    \n",
    "    #configuration data\n",
    "    configData = dict()\n",
    "    list_submissionData = list() \n",
    "    compTrueRes = pd.DataFrame()\n",
    "    scores = list()\n",
    "    \n",
    "    #Define the class constructor    \n",
    "    def __init__(self, configPath='', TeamID=''):\n",
    "               \n",
    "        #Set parameters based on inputs\"        \n",
    "        self.configPath = Path(configPath)    \n",
    "        self.TeamID = TeamID\n",
    "        \n",
    "        #if config file exists, else error out\n",
    "        if self.configPath.is_file():\n",
    "\n",
    "            #load scoring config data \n",
    "            self.load_scoring_config()\n",
    "                             \n",
    "            #load data\n",
    "            self.load_scoring_data()\n",
    "            \n",
    "            if self.configData['score_metrics']['score_answers']['required']:\n",
    "                #run scoring_answers\n",
    "                self.score_answers()\n",
    "                print(self.scores)\n",
    "                \n",
    "                \n",
    "            if self.configData['score_metrics']['score_code']['required']:\n",
    "                #run scoring_answers\n",
    "                #self.score_code()\n",
    "                print(2)\n",
    "                \n",
    "        else:\n",
    "            #error out with print statment\n",
    "            print(\"Error: Config File does not exist\")\n",
    "            \n",
    "     \n",
    "    def load_scoring_config(self): #A function to load configuration file\n",
    "        \n",
    "        #if file exist load configuration json into self.configData\n",
    "        with open(str(self.configPath)) as json_data:\n",
    "            self.configData = json.load(json_data)\n",
    "\n",
    "        #make sure that scoring config data is present\n",
    "        self.check_scoring_config()\n",
    "        \n",
    "        \n",
    "    def load_scoring_data(self): #A function to load submission score data\n",
    "        \n",
    "        # load Competion data\n",
    "        if Path(self.configData['score_metrics']['solution_file']).is_file():\n",
    "            self.compTrueRes = pd.read_csv(self.configData['score_metrics']['solution_file'])\n",
    "        else:\n",
    "            print(\"Error: solution_file path or name not correct\")\n",
    "           \n",
    "        # load team submissions data\n",
    "        if Path(self.configData['score_metrics']['submission_dir'] + self.TeamID + \"/\").is_dir(): #check to make sure dir exists\n",
    "            \n",
    "            p = Path(self.configData['score_metrics']['submission_dir'] + self.TeamID + \"/Submission/\")\n",
    "            for x in p.iterdir(): #load data in dir into list, data is in pd format\n",
    "                self.list_submissionData.append([str(x), pd.read_csv(x)])\n",
    "            \n",
    "\n",
    "    def check_scoring_config(self): #A function to check the structure of the configuration data\n",
    "        \n",
    "        #check the file to make sure we have the key feilds needed\n",
    "        if 'score_metrics' not in self.configData:\n",
    "            #if no score metrics, print error\n",
    "            print(\"Error: score_metrics not in Config File\")\n",
    "    \n",
    "    \n",
    "    def score_answers(self): #A function to compare submitted answers against true competition answers\n",
    "        \n",
    "        #score each submission, calculate metrics of interest\n",
    "        for d in self.list_submissionData:\n",
    "            #create a list to hold results\n",
    "            \n",
    "            if self.configData['score_metrics']['Classification']['accuracy']['required']:\n",
    "                if self.configData['score_metrics']['Classification']['accuracy']['type'].lower() == 'scikit-learn': #use scikit-learn classification accuracy\n",
    "                \n",
    "                    from sklearn.metrics import accuracy_score as score\n",
    "                    \n",
    "                    #convert df to numpy array and score via scikit-learn accuracy_score\n",
    "                    yTrue = self.compTrueRes.as_matrix()\n",
    "                    yPred = d[1].as_matrix()\n",
    "                    s = score(yTrue, yPred)\n",
    "                    \n",
    "                    #output result\n",
    "                    self.scores.append([d[0],'accuracy', s])\n",
    "                    \n",
    "                \n",
    "            if self.configData['score_metrics']['Classification']['average_precision']['required']:\n",
    "                if self.configData['score_metrics']['Classification']['average_precision']['type'].lower() == \"scikit-learn\": #use scikit-learn classification average_precision\n",
    "                                    \n",
    "                    from sklearn.metrics import average_precision_score as score\n",
    "                    \n",
    "                    #convert df to numpy array and score via scikit-learn average_precision\n",
    "                    yTrue = self.compTrueRes.as_matrix()\n",
    "                    yPred = d[1].as_matrix()\n",
    "                    s = score(yTrue, yPred)\n",
    "                    \n",
    "                    #output result\n",
    "                    self.scores.append([d[0],'average_precision', s])\n",
    "                    \n",
    "            if self.configData['score_metrics']['Classification']['f1']['required']:\n",
    "                if self.configData['score_metrics']['Classification']['f1']['type'].lower() == \"scikit-learn\": #use scikit-learn classification f1\n",
    "                    \n",
    "                    from sklearn.metrics import f1_score as score\n",
    "                    \n",
    "                    #convert df to numpy array and score via scikit-learn f1_score\n",
    "                    yTrue = self.compTrueRes.as_matrix()\n",
    "                    yPred = d[1].as_matrix()\n",
    "                      \n",
    "                    if self.configData['score_metrics']['Classification']['f1']['average'].lower() == \"micro\":\n",
    "                        s = score(yTrue, yPred, average='micro')\n",
    "                        self.scores.append([d[0],'f1_micro', s])\n",
    "                    elif self.configData['score_metrics']['Classification']['f1']['average'].lower() == \"macro\":\n",
    "                        s = score(yTrue, yPred, average='macro')\n",
    "                        self.scores.append([d[0],'f1_macro', s])\n",
    "                    elif self.configData['score_metrics']['Classification']['f1']['average'].lower() == \"weighted\":\n",
    "                        s = score(yTrue, yPred, average='weighted')\n",
    "                        self.scores.append([d[0],'f1_weighted', s])\n",
    "                    elif self.configData['score_metrics']['Classification']['f1']['average'].lower() == \"samples\":\n",
    "                        s = score(yTrue, yPred, average='samples')\n",
    "                        self.scores.append([d[0],'f1_samples', s])\n",
    "                    else:\n",
    "                        s = score(yTrue, yPred, average='binary')\n",
    "                        self.scores.append([d[0],'f1_binary', s])\n",
    "                        \n",
    "\n",
    "            if self.configData['score_metrics']['Classification']['log_loss']['required']:\n",
    "                if self.configData['score_metrics']['Classification']['log_loss']['type'].lower() == \"scikit-learn\": #use scikit-learn classification neg_log_loss\n",
    "                    \n",
    "                    from sklearn.metrics import log_loss as score\n",
    "                    \n",
    "                    #convert df to numpy array and score via scikit-learn log_loss\n",
    "                    yTrue = self.compTrueRes.as_matrix()\n",
    "                    yPred = d[1].as_matrix()\n",
    "                    s = score(yTrue, yPred)\n",
    "                    \n",
    "                    #output result\n",
    "                    self.scores.append([d[0],'log_loss', s])\n",
    "\n",
    "            if self.configData['score_metrics']['Classification']['precision']['required']:\n",
    "                if self.configData['score_metrics']['Classification']['precision']['type'].lower() == \"scikit-learn\": #use scikit-learn classification precision\n",
    "                    \n",
    "                    from sklearn.metrics import precision_score as score\n",
    "                    \n",
    "                    #convert df to numpy array and score via scikit-learn precision\n",
    "                    yTrue = self.compTrueRes.as_matrix()\n",
    "                    yPred = d[1].as_matrix()\n",
    "                    s = score(yTrue, yPred)\n",
    "                    \n",
    "                    #output result\n",
    "                    self.scores.append([d[0],'precision', s])\n",
    "\n",
    "            if self.configData['score_metrics']['Classification']['recall']['required']:\n",
    "                if self.configData['score_metrics']['Classification']['recall']['type'].lower() == \"scikit-learn\": #use scikit-learn classification recall\n",
    "                    \n",
    "                    from sklearn.metrics import recall_score as score\n",
    "                    \n",
    "                    #convert df to numpy array and score via scikit-learn recall\n",
    "                    yTrue = self.compTrueRes.as_matrix()\n",
    "                    yPred = d[1].as_matrix()\n",
    "                    s = score(yTrue, yPred)\n",
    "                    \n",
    "                    #output result\n",
    "                    self.scores.append([d[0],'recall', s])\n",
    "\n",
    "            if self.configData['score_metrics']['Classification']['roc_auc']['required']:    \n",
    "                if self.configData['score_metrics']['Classification']['roc_auc']['type'].lower() == \"scikit-learn\": #use scikit-learn classification roc_auc\n",
    "                    \n",
    "                    from sklearn.metrics import roc_auc_score as score\n",
    "                    \n",
    "                    #convert df to numpy array and score via scikit-learn roc_auc\n",
    "                    yTrue = self.compTrueRes.as_matrix()\n",
    "                    yPred = d[1].as_matrix()\n",
    "                    s = score(yTrue, yPred)\n",
    "                    \n",
    "                    #output result\n",
    "                    self.scores.append([d[0],'roc_auc', s])\n",
    "\n",
    "            if self.configData['score_metrics']['Clustering']['adjusted_rand_score']['required']:\n",
    "                if self.configData['score_metrics']['Clustering']['adjusted_rand_score']['type'].lower() == \"scikit-learn\": #use scikit-learn Clustering adjusted_rand_score\n",
    "                    \n",
    "                    from sklearn.metrics import adjusted_rand_score as score\n",
    "                    \n",
    "                    #convert df to numpy array and score via scikit-learn adjusted_rand_score\n",
    "                    #yTrue = self.compTrueRes.as_matrix()\n",
    "                    #yPred = d[1].as_matrix()\n",
    "                    #s = score(yTrue, yPred)\n",
    "                    \n",
    "                    #output result\n",
    "                    #self.scores.append([d[0],'adjusted_rand_score', s])\n",
    "\n",
    "            if self.configData['score_metrics']['Regression']['mean_absolute_error']['required']:\n",
    "                if self.configData['score_metrics']['Regression']['mean_absolute_error']['type'].lower() == \"scikit-learn\": #use scikit-learn Regression neg_mean_absolute_error\n",
    "                    \n",
    "                    from sklearn.metrics import mean_absolute_error as score\n",
    "                    \n",
    "                    #convert df to numpy array and score via scikit-learn mean_absolute_error\n",
    "                    yTrue = self.compTrueRes.as_matrix()\n",
    "                    yPred = d[1].as_matrix()\n",
    "                    s = score(yTrue, yPred)\n",
    "                    \n",
    "                    #output result\n",
    "                    self.scores.append([d[0],'mean_absolute_error', s])\n",
    "\n",
    "            if self.configData['score_metrics']['Regression']['mean_squared_error']['required']:\n",
    "                if self.configData['score_metrics']['Regression']['mean_squared_error']['type'].lower() == \"scikit-learn\": #use scikit-learn Regression neg_mean_absolute_error\n",
    "                    \n",
    "                    from sklearn.metrics import mean_squared_error as score\n",
    "                    \n",
    "                    #convert df to numpy array and score via scikit-learn mean_squared_error\n",
    "                    yTrue = self.compTrueRes.as_matrix()\n",
    "                    yPred = d[1].as_matrix()\n",
    "                    s = score(yTrue, yPred)\n",
    "                    \n",
    "                    #output result\n",
    "                    self.scores.append([d[0],'mean_squared_error', s])\n",
    "                    \n",
    "            if self.configData['score_metrics']['Regression']['median_absolute_error']['required']:\n",
    "                if self.configData['score_metrics']['Regression']['median_absolute_error']['type'].lower() == \"scikit-learn\": #use scikit-learn Regression neg_median_absolute_error\n",
    "                    \n",
    "                    from sklearn.metrics import median_absolute_error as score\n",
    "                    \n",
    "                    #convert df to numpy array and score via scikit-learn median_absolute_error\n",
    "                    yTrue = self.compTrueRes.as_matrix()\n",
    "                    yPred = d[1].as_matrix()\n",
    "                    s = score(yTrue, yPred)\n",
    "                    \n",
    "                    #output result\n",
    "                    self.scores.append([d[0],'median_absolute_error', s])\n",
    "\n",
    "            if self.configData['score_metrics']['Regression']['r2']['required']:\n",
    "                if self.configData['score_metrics']['Regression']['r2']['type'].lower() == \"scikit-learn\": #use scikit-learn Regression r2\n",
    "                    \n",
    "                    from sklearn.metrics import r2_score as score\n",
    "                    \n",
    "                    #convert df to numpy array and score via scikit-learn r2_score\n",
    "                    yTrue = self.compTrueRes.as_matrix()\n",
    "                    yPred = d[1].as_matrix()\n",
    "                    s = score(yTrue, yPred)\n",
    "                    \n",
    "                    #output result\n",
    "                    self.scores.append([d[0],'r2', s])\n",
    "                \n",
    "                \n",
    "    def score_code(self): #A function to run submitted code and compare answers against true competition answers \n",
    "        #code\n",
    "        print(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub1.csv', 'accuracy', 0.98241590214067276], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub1.csv', 'average_precision', 0.98574595674529486], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub1.csv', 'f1_binary', 0.97641025641025636], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub1.csv', 'log_loss', 0.60733322406953139], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub1.csv', 'precision', 1.0], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub1.csv', 'recall', 0.95390781563126248], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub1.csv', 'roc_auc', 0.9769539078156313], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub1.csv', 'mean_absolute_error', 0.017584097859327217], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub1.csv', 'mean_squared_error', 0.017584097859327217], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub1.csv', 'median_absolute_error', 0.0], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub1.csv', 'r2', 0.92547765493904988], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub2.csv', 'accuracy', 0.99923547400611623], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub2.csv', 'average_precision', 0.99938025898892591], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub2.csv', 'f1_binary', 0.99899699097291883], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub2.csv', 'log_loss', 0.026405792350850141], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub2.csv', 'precision', 1.0], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub2.csv', 'recall', 0.99799599198396793], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub2.csv', 'roc_auc', 0.99899799599198391], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub2.csv', 'mean_absolute_error', 0.00076452599388379206], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub2.csv', 'mean_squared_error', 0.00076452599388379206], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub2.csv', 'median_absolute_error', 0.0], ['/home/john/Projects/Dabble/TeamCode/Team0/Submission/Team0_sub2.csv', 'r2', 0.99675989804082821]]\n"
     ]
    }
   ],
   "source": [
    "#create scoring object\n",
    "dabbleScoreObj = ScoringObject(\"/home/john/Projects/Dabble/Configs/config_comp0.json\",\"Team0\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['/home/john/Projects/Dabble/TeamCode/Team1/Submission/Team0_sub1.csv', 'accuracy', 0.99159021406727832], ['/home/john/Projects/Dabble/TeamCode/Team1/Submission/Team0_sub1.csv', 'average_precision', 0.99318284887818453], ['/home/john/Projects/Dabble/TeamCode/Team1/Submission/Team0_sub1.csv', 'f1_binary', 0.98885511651469093], ['/home/john/Projects/Dabble/TeamCode/Team1/Submission/Team0_sub1.csv', 'log_loss', 0.29046371585934161], ['/home/john/Projects/Dabble/TeamCode/Team1/Submission/Team0_sub1.csv', 'precision', 1.0], ['/home/john/Projects/Dabble/TeamCode/Team1/Submission/Team0_sub1.csv', 'recall', 0.97795591182364727], ['/home/john/Projects/Dabble/TeamCode/Team1/Submission/Team0_sub1.csv', 'roc_auc', 0.98897795591182369], ['/home/john/Projects/Dabble/TeamCode/Team1/Submission/Team0_sub1.csv', 'mean_absolute_error', 0.0084097859327217118], ['/home/john/Projects/Dabble/TeamCode/Team1/Submission/Team0_sub1.csv', 'mean_squared_error', 0.0084097859327217118], ['/home/john/Projects/Dabble/TeamCode/Team1/Submission/Team0_sub1.csv', 'median_absolute_error', 0.0], ['/home/john/Projects/Dabble/TeamCode/Team1/Submission/Team0_sub1.csv', 'r2', 0.96435887844911083]]\n"
     ]
    }
   ],
   "source": [
    "dabbleScoreObj = ScoringObject(\"/home/john/Projects/Dabble/Configs/config_comp0.json\",\"Team1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub1.csv', 'accuracy', 0.99617737003058104], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub1.csv', 'average_precision', 0.9957605179778517], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub1.csv', 'f1_binary', 0.994994994994995], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub1.csv', 'log_loss', 0.13203079569330681], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub1.csv', 'precision', 0.99399999999999999], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub1.csv', 'recall', 0.99599198396793587], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub1.csv', 'roc_auc', 0.99614185106925834], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub1.csv', 'mean_absolute_error', 0.0038226299694189602], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub1.csv', 'mean_squared_error', 0.0038226299694189602], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub1.csv', 'median_absolute_error', 0.0], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub1.csv', 'r2', 0.98379949020414126], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub2.csv', 'accuracy', 0.99541284403669728], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub2.csv', 'average_precision', 0.99628155393355522], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub2.csv', 'f1_binary', 0.99395161290322576], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub2.csv', 'log_loss', 0.15843475410509583], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub2.csv', 'precision', 1.0], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub2.csv', 'recall', 0.9879759519038076], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub2.csv', 'roc_auc', 0.9939879759519038], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub2.csv', 'mean_absolute_error', 0.0045871559633027525], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub2.csv', 'mean_squared_error', 0.0045871559633027525], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub2.csv', 'median_absolute_error', 0.0], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub2.csv', 'r2', 0.98055938824496958], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub3.csv', 'accuracy', 0.98623853211009171], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub3.csv', 'average_precision', 0.98259187620889743], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub3.csv', 'f1_binary', 0.98228346456692917], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub3.csv', 'log_loss', 0.47531526594964607], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub3.csv', 'precision', 0.96518375241779497], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub3.csv', 'recall', 1.0], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub3.csv', 'roc_auc', 0.9888751545117429], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub3.csv', 'mean_absolute_error', 0.013761467889908258], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub3.csv', 'mean_squared_error', 0.013761467889908258], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub3.csv', 'median_absolute_error', 0.0], ['/home/john/Projects/Dabble/TeamCode/Team2/Submission/Team0_sub3.csv', 'r2', 0.94167816473490862]]\n"
     ]
    }
   ],
   "source": [
    "dabbleScoreObj = ScoringObject(\"/home/john/Projects/Dabble/Configs/config_comp0.json\",\"Team2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
